---
title: "The ecological consequences of historical redlining"
format:
  html:
    embed-resources: true
---

This executable notebook provides an opening example to illustrate a cloud-native workflow in both R and Python. Pedagogy research emphasizes the importance of “playing the whole game” before breaking down every pitch and hit. We intentionally focus on powerful high-level tools (STAC API, COGs, datacubes) to illustrate how a few chunks of code can perform a task that would be far slower and more verbose in a traditional file-based, download-first workflow.

# Required packages 

```{r echo = FALSE, message = FALSE}
library(rstac)
library(gdalcubes)
library(stars)
library(tmap)
library(dplyr)
library(lubridate)
gdalcubes::gdalcubes_options(parallel = TRUE)
```

# Data discovery

The first step in many workflows involves discovering individual spatial data files covering the space, time, and variables of interest. Here we use a STAC Catalog API to recover a list of candidate data. We dig deeper into how this works and what it returns in later recipes. This example searches for images in a lon-lat bounding box from a collection of Cloud-Optimized-GeoTIFF (COG) images taken by Sentinel2 satellite mission. This function will not download any imagery, it merely gives us a list of metadata about available images, including the access URLs.

A COG is a regular GeoTIFF file that is intended to be hosted on an HTTP file server. Its internal organization enables more efficient workflows in the cloud.

Sentinel-2 is an Earth observation mission from the Copernicus Programme that acquires optical imagery at high spatial resolution (10 m to 60 m) over land and coastal waters.  It collects data in bands, which are different wavelengths of reflected light from the Earth's surface and atmosphere.  Different bands can be used to study different aspects of the environment and can be used to calculate a range of [indexes](https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel/sentinel-2/#remote-sensing-indices) that are known to represent a key process.    

This tutorial focuses on the city of San Francisco.  It is contained in a box defined by the decimal degrees north and decimal degrees east

```{r}
box <- c(xmin = -122.51, 
         ymin = 37.71, 
         xmax = -122.36, 
         ymax = 37.81) 
```

Only summer dates will be used because we want to ensure that leaves are on the trees.  We end up averaging across all values in this time window.  The "T" separates the date and time component.  The "Z" is the time zone (Z by itself is UTC).

```{r}
start_date <- "2022-06-01T00:00:00Z"
end_date <- "2022-08-01T00:00:00Z"
```

Explore the STAC catalog at <https://stacindex.org/catalogs/earth-search#> to learn more about the data.

We are focusing on the collection within the catalog that includes Sentinel-2 data (sentinel-2-l2a).  

The search includes the box, dates, and a filter for clouds.  The post request sends the search query to the server where the catalog is hosted.  It returns an object that includes the list of images that match our criteria.  

```{r}
items <- stac("https://earth-search.aws.element84.com/v1/") |>
  stac_search(collections = "sentinel-2-l2a",
              bbox = box,
              datetime = paste(start_date, end_date, sep="/"),
              limit = 100) |>
  ext_query("eo:cloud_cover" < 20) |>
  post_request()
```

The items object provides the available images (features) and bands (assets).

```{r}
items
```

We pass this list of images to a high-level utility (gdalcubes in R, odc.stac in python) that will do all of the heavy lifting. Using the URLs and metadata provided by STAC, these functions can extract only our data of interest (given by the bounding box) without downloading unnecessary regions or bands. While streaming the data, these functions will also reproject it into the desired coordinate reference system – (an often costly operation to perform in R) and can potentially resample or aggregate the data to a desired spatial resolution. (The R code will also resample from images in overlapping areas to replace pixels masked by clouds)

The function `stac_image_collection` creates an image collection from a STAC API collection response.  The asset names are the different bands within each image.  In this case, by exploring <https://stacindex.org/catalogs/earth-search#/43bjKKcJQfxYaT1ir3Ep6uENfjEoQrjkzhd2?t=3>, we learned that `red` is the red band, `nir08` is the near-infrared band, and `scl` describes the state of clouds in the pixel.

```{r}
col <- stac_image_collection(items$features, asset_names = c("red", "nir08", "scl"))
```

Data cube views define the shape of a cube, i.e., the spatiotemporal extent, resolution, and spatial reference system (srs). They are used to access image collections as on-demand data cubes. The data cube will filter images based on the view's extent, read image data at the defined resolution, and warp / reproject images to the target srs automatically.


```{r}
cube <- cube_view(srs ="EPSG:4326",
                  extent = list(t0 = start_date, 
                                t1 = end_date,
                                left = box[1], 
                                right = box[3],
                                top = box[4], 
                                bottom = box[2]),
                  dx = 0.0001, 
                  dy = 0.0001, 
                  dt = "P1D",
                  aggregation = "median", 
                  resampling = "average")
```

You likely had warnings associated with making sure the cube has regular-sized boxes and time chunks. This is a good feature because it helps you get the right size cube and will always widen the cube to include what you asked for.

The function `image_mask` creates an image mask based on values in a band.  In our data cube, only pixels in the image that are not in this mask are used.

By exploring https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/scene-classification/>, we found that `scl` values of 3, 8, and 9 correspond to clouds and cloud shadows, and we want to filter out any pixels with those values.

```{r}
mask <- image_mask("scl", values=c(3, 8, 9)) # mask clouds and cloud shadows
```

Next, `raster_cube` creates a proxy data cube, which loads data from a given image collection according to a data cube view and filtering mask.  

```{r}
data <-  raster_cube(image_collection = col, 
                     view = cube, 
                     mask = mask)
```

We can do arbitrary calculations on this data as well. Here we calculate NDVI, a widely used measure of greenness that can be used to determine tree cover. (Note that the R example uses lazy evaluation, and can thus perform these calculations while streaming).  

The function `select_bands` selects the defined bands.  In our case, the band selected also occurred at the call to `stac_image_collection` above, so this selection only removes the `scl` band.

The function `apply_pixel` applies the band-wise calculation to each pixel (e.g., calculates NDVI within each pixel in each image). The function `reduce_time` finds the time dimension and applies the expression to collapse it down to a single value per x-y location.

```{r}
ndvi <- data |>
  select_bands(bands = c("red", "nir08")) |>
  apply_pixel(expr = "(nir08-red)/(nir08+red)", names = "NDVI") |>
  reduce_time(expr = c("mean(NDVI)"), names = "NDVI_mean")
```

Now, convert the NDVI object to a "stars" object so that it can more easily be visualized on a map.  A `stars` object is also a data cube with a slightly different format that plays well with mapping functions.  This step will take a while to run because it is the step where data is transferred from the remote server to your computer. The speed at which this step runs will depend on your Wi-Fi speed or the network connection for the computer you are running it on.

```{r}
ndvi_stars <- st_as_stars(ndvi)
```

Do not worry if you get the following warning: `Warning: GDAL Message 1: The dataset has several variables that could be identified as vector fields, but not all share the same primary dimension. Consequently they will be ignored.`

Now, you can plot the result. The long rectangle of Golden Gate Park is clearly visible in the Northwest.

First, define the scales for the image using the `viridisLite::mako` palate. 

```{r}
mako <- tm_scale_continuous(values = viridisLite::mako(30))
```

Use the `map` package to visualize the image. `tm_shape` defines the map's shape (geographical shape). `tm_raster` adds the raster data and uses the scale we defined above. Note that because of running `reduce_time` above, our raster only has one value per location. `tmap` works like `ggplot2`, where layers are added to a base plot.  

```{r}
tm_shape(shp = ndvi_stars) + 
  tm_raster(col = "NDVI_mean", col.scale = mako)
```

# From NDVI to environmental justice

We examine the present-day impact of historic “red-lining” of US cities during the Great Depression using data from the Mapping Inequality project. All though this racist practice was banned by federal law under the Fair Housing Act of 1968, the systemic scars of that practice are still so deeply etched on our landscape that they remain visible from space – “red-lined” areas (graded “D” under the racist HOLC scheme) show systematically lower greenness than predominately-white neighborhoods (Grade “A”). Trees provide many benefits, from mitigating urban heat to biodiversity, real-estate value, to health.

Information about HOLC Schema from Wikipedia:

- The Home Owners' Loan Corporation (HOLC) was a government-sponsored corporation created as part of the New Deal
- The HOLC created a housing appraisal system of color-coded maps that categorized the riskiness of lending to households in different neighborhoods
- Lower grades = less opportunity for loans but lower grades also = minority communities
- Red-lining: A discriminatory practice in which financial services are withheld from neighborhoods that have significant numbers of racial and ethnic minorities
- It was outlawed in 1968 following the passage of the civil rights bill. 

## Zonal statistics

In addition to large-scale raster data such as satellite imagery, the analysis of vector shapes such as polygons showing administrative regions is a central component of spatial analysis and particularly important to spatial social sciences. The red-lined areas of the 1930s are one example of spatial vectors. One common operation is to summarize the values of all pixels falling within a given polygon, e.g. computing the average greenness (NDVI).

Polygons with the grades for different cities across the U.S. are found at <https://dsl.richmond.edu/panorama/redlining/static/mappinginequality.gpkg>

The function `st_read()` reads the polygon file from the remote server. The file includes polygons of different administrated regions along with associated descriptive data. It converts the data to an SF table.  Running `st_make_valid` is good practice because it ensures the data are correctly formatted.

From Wikipedia: Simple Features (officially Simple Feature Access) is a set of standards that specify a common storage and access model of geographic features made of mostly two-dimensional geometries (point, line, polygon, multi-point, multi-line, etc.) used by geographic databases and geographic information systems. It is formalized by both the Open Geospatial Consortium (OGC) and the International Organization for Standardization (ISO).

Finally, the polygons are filtered to only include those from our focal city (San Francisco).

```{r}
focal_city <- "San Francisco"

sf <- "/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/mappinginequality.gpkg" |>
  st_read() |>
  st_make_valid() |> 
  filter(city == focal_city)
```

We can use the `gdalcubes` package to extract data from the cube for each geometry (polygon).  In this case, we are using the sf geometry from above. We are calculating the mean in each polygon and telling it to ignore the time component since we have already reduced the time dimension.  

This step will take a while to run because data is transferred from the remote server to your computer. The speed at which this step runs will depend on your Wi-Fi speed or the network connection for the computer you are running it on.

```{r}
poly <- ndvi |> 
  extract_geom(sf, FUN = mean, reduce_time = TRUE)
```

Add the new mean data to the sf object.

```{r}
sf$NDVI <- poly$NDVI_mean
```

We plot the underlying NDVI as well as the average NDVI of each polygon, along with its textual grade, using `tmap.` Note that “A” grades tend to be the darkest green (high NDVI), while “D” grades are frequently the least green. (Regions not zoned for housing at the time of the 1937 housing assessment are not displayed as polygons.)

```{r}
tm_shape(shp = ndvi_stars) + 
  tm_raster(col = "NDVI_mean", col.scale = mako) +
  tm_shape(shp = sf) + 
  tm_polygons(fill = 'NDVI', fill.scale = tm_scale_continuous(values = "brewer.greens")) +
  tm_shape(shp = sf) + 
  tm_text(text = "grade", col="darkblue", size=0.6) +
  tm_legend_hide()
```

## Are historically redlined areas less green?

Redlining was outlawed in 1968 following the passage of the civil rights bill.  Over 50 years later, can we still see its legacy?

To answer the question, we want to convert the sf object to a data frame (tibble) and use familiar dpylr functions to calculate the statistics for each grade.

```{r}
sf |> 
  as_tibble() |>
  group_by(grade) |> 
  summarise(ndvi = mean(NDVI), 
            sd = sd(NDVI)) |>
  knitr::kable()
```

# Combined code

As you look over the combined code, consider the following questions:

1) What would you change to adapt to a new location?  Here is a list of the cities in the database.

```{r}
"/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/mappinginequality.gpkg" |>
  st_read() |>
  st_make_valid() |> 
  as_tibble() |> 
  distinct(city)
```

2) What would you change to adapt to the new remote sensing index? (see <https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel/sentinel-2/#remote-sensing-indices>)

## Define the spatial and temporal domain

```{r}
focal_city <- "San Francisco"
box <- c(xmin = -122.51, 
         ymin = 37.71, 
         xmax = -122.36, 
         ymax = 37.81) 
start_date <- "2022-06-01T00:00:00Z"
end_date <- "2022-08-01T00:00:00Z"
```

## Acquire and Process Raster Data

```{r}
items <-
  stac("https://earth-search.aws.element84.com/v1/") |>
  stac_search(collections = "sentinel-2-l2a",
              bbox = box,
              datetime = paste(start_date, end_date, sep="/"),
              limit = 100) |>
  ext_query("eo:cloud_cover" < 20) |>
  post_request()

col <- stac_image_collection(items$features, asset_names = c("red", "nir08", "scl"))

cube <- cube_view(srs ="EPSG:4326",
                  extent = list(t0 = start_date, t1 = end_date,
                                left = box[1], right = box[3],
                                top = box[4], bottom = box[2]),
                  dx = 0.0001, dy = 0.0001, dt = "P1D",
                  aggregation = "median", resampling = "average")

mask <- image_mask("scl", values=c(3, 8, 9)) # mask clouds and cloud shadows

data <-  raster_cube(image_collection = col, view = cube, mask = mask)

ndvi <- data |>
  select_bands(bands = c("red", "nir08")) |>
  apply_pixel(expr = "(nir08-red)/(nir08+red)", names = "NDVI") |>
  reduce_time(expr = c("mean(NDVI)"), names = "NDVI_mean")

ndvi_stars <- st_as_stars(ndvi)
```

## Acquire and Process Polygon Data

```{r}
sf <- "/vsicurl/https://dsl.richmond.edu/panorama/redlining/static/mappinginequality.gpkg" |>
  st_read() |>
  st_make_valid() |> 
  filter(city == focal_city) 
  
poly <- ndvi |> 
  extract_geom(sf, FUN = mean, reduce_time = TRUE)

sf$NDVI <- poly$NDVI_mean
```

## Generate map

```{r}
tm_shape(shp = ndvi_stars) + 
  tm_raster(col = "NDVI_mean", col.scale = mako) +
  tm_shape(shp = sf) + 
  tm_polygons(fill = 'NDVI', fill.scale = tm_scale_continuous(values = "brewer.greens")) +
  tm_shape(shp = sf) + 
  tm_text(text = "grade", col="darkblue", size=0.6) +
  tm_legend_hide()
```

## Data Analysis 

```{r}
sf |> 
  as_tibble() |>
  group_by(grade) |> 
  summarise(ndvi = mean(NDVI), 
            sd = sd(NDVI)) |>
  knitr::kable()
```

# Citation

This tutorial is an extension of the tutorial created by Carl Boettiger and Millie Chapman at the University of California - Berkeley. <https://boettiger-lab.github.io/nasa-topst-env-justice/tutorials/R/1-intro-R.html>

# Attribution

Include citations of any AI-generated assistance or discussion with classmates (per policy in the syllabus). Proper documentation of AI-generated assistance includes the prompt, the source (e.g., ChatGPT), and the significant parts of the response. Proper documentation of discussion with classmates includes listing their names and the components discussed.
